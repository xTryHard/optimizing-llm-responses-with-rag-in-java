package com.theitdojo.optimizing_llm_responses_with_rag_in_java.services;

import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.chat.client.advisor.MessageChatMemoryAdvisor;
import org.springframework.ai.chat.memory.ChatMemory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.core.io.Resource;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;

import java.util.stream.Stream;

@Service
public class ChatAssistantService implements ChatAssistant {
    private final ChatClient chatClient;

    public ChatAssistantService(ChatClient.Builder builder,
                                @Value("classpath:/system-prompt.md") Resource systemPrompt,
                                ChatMemory chatMemory) {
        this.chatClient = builder
                .defaultSystem(systemPrompt)
                .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build())
                .build();
    }

    @Override
    public String getResponse(String conversationId, String message) {
        return this.chatClient.prompt()
                .advisors(advisor -> advisor.param(ChatMemory.CONVERSATION_ID, conversationId))
                .user(message)
                .call()
                .content();
    }

    @Override
    public Stream<String> streamResponse(String conversationId, String message) {
        return chatClient.prompt()
                .advisors(advisor -> advisor.param(ChatMemory.CONVERSATION_ID, conversationId))
                .user(message)
                .stream()
                .content()
                .toStream();
    }

    @Override
    public Flux<String> askQuestionWithContext(String conversationId, String question) {
        // TODO: Implementar la lÃ³gica de RAG en un futuro ejercicio.
        // Por ahora, se ignora el contexto y se llama directamente al modelo.
        return chatClient.prompt()
                .user(question)
                .advisors(advisor -> advisor.param(ChatMemory.CONVERSATION_ID, conversationId))
                .stream()
                .content();
    }
}
